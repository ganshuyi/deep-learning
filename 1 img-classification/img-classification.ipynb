{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.functional import normalize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 미리 작성된 코드들은 수정할 수 없으며, 이외의 코드를 작성하시면 됩니다.\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    # 모델의 코드는 여기서 작성해주세요\n",
        "\n",
        "    def __init__(self, drop_prob):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(32*32*3, 256)\n",
        "        self.linear2 = nn.Linear(256, 128)\n",
        "        self.linear3 = nn.Linear(128, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.linear1(x)\n",
        "        a1 = self.activation(z1)\n",
        "        a1 = self.dropout(a1)\n",
        "\n",
        "        z2 = self.linear2(a1)\n",
        "        a2 = self.activation(z2)\n",
        "        a2 = self.dropout(a2)\n",
        "\n",
        "        output = self.linear3(a2)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 학습코드는 모두 여기서 작성해주세요\n",
        "\n",
        "    # normalization\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "    \n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                                 train=True,\n",
        "                                                 transform=transform,\n",
        "                                                 download=True)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                                train=False,\n",
        "                                                transform=transform,\n",
        "                                                download=True)\n",
        "\n",
        "    model = Classifier(0.1).to(device).train()\n",
        "\n",
        "    #===================================\n",
        "    #            train code \n",
        "    #===================================\n",
        "    batch_size = 128\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    import torch.optim as optim\n",
        "    optimizer = optim.SGD(params=model.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    epochs = 90\n",
        "    lmb = 0.003\n",
        "\n",
        "    train_avg_costs = []\n",
        "    \n",
        "    total_batch_num = len(train_dataloader)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      avg_cost = 0.0\n",
        "      model.train()\n",
        "      \n",
        "      for b_x, b_y in train_dataloader:\n",
        "        b_x = b_x.view(-1, 32*32*3).to(device)\n",
        "        logits = model(b_x) # forward propagation\n",
        "        loss = criterion(logits, b_y.to(device)) # get cost\n",
        "\n",
        "        # L2 regularization\n",
        "        reg = model.linear1.weight.pow(2.0).sum()\n",
        "        reg += model.linear2.weight.pow(2.0).sum()\n",
        "        reg += model.linear3.weight.pow(2.0).sum()\n",
        "        loss += lmb * reg / len(b_x) / 2.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # backward propagation\n",
        "        optimizer.step() # update parameters\n",
        "\n",
        "        avg_cost += loss / total_batch_num\n",
        "      train_avg_costs.append(avg_cost.detach().cpu())\n",
        "      print('Epoch : {} / {}, cost : {}'.format(epoch+1, epochs, avg_cost))\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), 'model.pt')  # 학습된 모델을 저장하는 코드입니다.\n"
      ],
      "metadata": {
        "id": "zifx8uJqvK24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dffcd59-4fc0-44dc-eabf-91282fc3640e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch : 1 / 90, cost : 2.1257476806640625\n",
            "Epoch : 2 / 90, cost : 1.9149439334869385\n",
            "Epoch : 3 / 90, cost : 1.837728500366211\n",
            "Epoch : 4 / 90, cost : 1.7863845825195312\n",
            "Epoch : 5 / 90, cost : 1.7506747245788574\n",
            "Epoch : 6 / 90, cost : 1.7176330089569092\n",
            "Epoch : 7 / 90, cost : 1.6888444423675537\n",
            "Epoch : 8 / 90, cost : 1.6650656461715698\n",
            "Epoch : 9 / 90, cost : 1.642798900604248\n",
            "Epoch : 10 / 90, cost : 1.6244865655899048\n",
            "Epoch : 11 / 90, cost : 1.606724739074707\n",
            "Epoch : 12 / 90, cost : 1.5892096757888794\n",
            "Epoch : 13 / 90, cost : 1.5739779472351074\n",
            "Epoch : 14 / 90, cost : 1.5605857372283936\n",
            "Epoch : 15 / 90, cost : 1.5446388721466064\n",
            "Epoch : 16 / 90, cost : 1.531154751777649\n",
            "Epoch : 17 / 90, cost : 1.5192245244979858\n",
            "Epoch : 18 / 90, cost : 1.5044276714324951\n",
            "Epoch : 19 / 90, cost : 1.494360327720642\n",
            "Epoch : 20 / 90, cost : 1.4819157123565674\n",
            "Epoch : 21 / 90, cost : 1.4747015237808228\n",
            "Epoch : 22 / 90, cost : 1.4608627557754517\n",
            "Epoch : 23 / 90, cost : 1.4536205530166626\n",
            "Epoch : 24 / 90, cost : 1.4388740062713623\n",
            "Epoch : 25 / 90, cost : 1.4303674697875977\n",
            "Epoch : 26 / 90, cost : 1.4191222190856934\n",
            "Epoch : 27 / 90, cost : 1.4137240648269653\n",
            "Epoch : 28 / 90, cost : 1.4032899141311646\n",
            "Epoch : 29 / 90, cost : 1.394100546836853\n",
            "Epoch : 30 / 90, cost : 1.3829333782196045\n",
            "Epoch : 31 / 90, cost : 1.3743035793304443\n",
            "Epoch : 32 / 90, cost : 1.364988088607788\n",
            "Epoch : 33 / 90, cost : 1.357466220855713\n",
            "Epoch : 34 / 90, cost : 1.34933602809906\n",
            "Epoch : 35 / 90, cost : 1.3410786390304565\n",
            "Epoch : 36 / 90, cost : 1.3316103219985962\n",
            "Epoch : 37 / 90, cost : 1.3242202997207642\n",
            "Epoch : 38 / 90, cost : 1.3150476217269897\n",
            "Epoch : 39 / 90, cost : 1.3063026666641235\n",
            "Epoch : 40 / 90, cost : 1.3050830364227295\n",
            "Epoch : 41 / 90, cost : 1.2931419610977173\n",
            "Epoch : 42 / 90, cost : 1.2880758047103882\n",
            "Epoch : 43 / 90, cost : 1.2823076248168945\n",
            "Epoch : 44 / 90, cost : 1.2715758085250854\n",
            "Epoch : 45 / 90, cost : 1.2696186304092407\n",
            "Epoch : 46 / 90, cost : 1.2578898668289185\n",
            "Epoch : 47 / 90, cost : 1.2551454305648804\n",
            "Epoch : 48 / 90, cost : 1.2474429607391357\n",
            "Epoch : 49 / 90, cost : 1.2394344806671143\n",
            "Epoch : 50 / 90, cost : 1.23401939868927\n",
            "Epoch : 51 / 90, cost : 1.2298356294631958\n",
            "Epoch : 52 / 90, cost : 1.2234439849853516\n",
            "Epoch : 53 / 90, cost : 1.2177468538284302\n",
            "Epoch : 54 / 90, cost : 1.2119935750961304\n",
            "Epoch : 55 / 90, cost : 1.202743411064148\n",
            "Epoch : 56 / 90, cost : 1.2043918371200562\n",
            "Epoch : 57 / 90, cost : 1.1928484439849854\n",
            "Epoch : 58 / 90, cost : 1.1845749616622925\n",
            "Epoch : 59 / 90, cost : 1.1801217794418335\n",
            "Epoch : 60 / 90, cost : 1.17971670627594\n",
            "Epoch : 61 / 90, cost : 1.1732782125473022\n",
            "Epoch : 62 / 90, cost : 1.169905185699463\n",
            "Epoch : 63 / 90, cost : 1.1609327793121338\n",
            "Epoch : 64 / 90, cost : 1.157766342163086\n",
            "Epoch : 65 / 90, cost : 1.1547820568084717\n",
            "Epoch : 66 / 90, cost : 1.1503593921661377\n",
            "Epoch : 67 / 90, cost : 1.1490833759307861\n",
            "Epoch : 68 / 90, cost : 1.1387124061584473\n",
            "Epoch : 69 / 90, cost : 1.13699471950531\n",
            "Epoch : 70 / 90, cost : 1.1293766498565674\n",
            "Epoch : 71 / 90, cost : 1.127128005027771\n",
            "Epoch : 72 / 90, cost : 1.1251589059829712\n",
            "Epoch : 73 / 90, cost : 1.1248846054077148\n",
            "Epoch : 74 / 90, cost : 1.1140128374099731\n",
            "Epoch : 75 / 90, cost : 1.1129660606384277\n",
            "Epoch : 76 / 90, cost : 1.1042485237121582\n",
            "Epoch : 77 / 90, cost : 1.10425865650177\n",
            "Epoch : 78 / 90, cost : 1.0998079776763916\n",
            "Epoch : 79 / 90, cost : 1.0975664854049683\n",
            "Epoch : 80 / 90, cost : 1.0924499034881592\n",
            "Epoch : 81 / 90, cost : 1.0894834995269775\n",
            "Epoch : 82 / 90, cost : 1.0854716300964355\n",
            "Epoch : 83 / 90, cost : 1.080986499786377\n",
            "Epoch : 84 / 90, cost : 1.0791698694229126\n",
            "Epoch : 85 / 90, cost : 1.0715781450271606\n",
            "Epoch : 86 / 90, cost : 1.0761504173278809\n",
            "Epoch : 87 / 90, cost : 1.0712968111038208\n",
            "Epoch : 88 / 90, cost : 1.0672061443328857\n",
            "Epoch : 89 / 90, cost : 1.0632535219192505\n",
            "Epoch : 90 / 90, cost : 1.060880184173584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r1BHoIEivAIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec6a7bd-fb00-4d98-ba11-893c25843c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test set : 52.5100%\n"
          ]
        }
      ],
      "source": [
        "# 학습된 모델의 성능을 평가하는 코드입니다.\n",
        "# 아래의 코드로 평가를 진행할 예정이므로 아래의 코드가 정상 동작 해야하며, 제출전 모델의 성능을 확인하시면 됩니다.\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                            train=False,\n",
        "                                            transform=transform,\n",
        "                                            download=True)\n",
        "\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
        "\n",
        "classifier = Classifier(0.1).to(device)\n",
        "classifier.load_state_dict(torch.load('model.pt'))\n",
        "classifier.eval()\n",
        "\n",
        "\n",
        "for data, label in test_dataloader:\n",
        "    data = data.view(-1, 32 * 32 * 3).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = classifier(data)\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        total = len(label)\n",
        "        correct = torch.eq(pred, label.to(device)).sum()\n",
        "\n",
        "        print(\"Accuracy on test set : {:.4f}%\".format(100 * correct / total))"
      ]
    }
  ]
}